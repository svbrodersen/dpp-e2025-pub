\section{Implementing prefix sum}
\subsection{Hillis Steele}
The hillis steele implementation can be seen below:
\begin{lstlisting}[language={futhark}]
def hillis_steele [n] (xs: [n]i32) : [n]i32 = 
  let m = ilog2 n
  in loop xs = copy xs for d in 0...m-1 do 
    let pow_2 = 1 << d
    in map (\i -> 
      if i - pow_2 >= 0 then 
        xs[i-(pow_2)] + xs[i] 
      else 
        xs[i]) (iota n)
\end{lstlisting}
This works quite intuitively as we are assuming our input is some power of 2.
First, we compute m via the provided ilog2 function. Then, in a loop given m we
map over all the indices. If an index is less than the current $2^d$, then we
simply copy the previous iteration result and otherwise we add
$xs[i-pow\_2]+xs[i]$.

This was tested as follows:
\begin{lstlisting}[language={futhark}]
-- Hillis test
-- ==
-- entry: test_hillis
-- nobench input { [0, 0, 1, 0, 0, 0, 0, 0] }
-- output { [0, 0, 1, 1, 1, 1, 1, 1] }
-- nobench input { [0, 1, 1, 0, 0, 0, 0, 0] }
-- output { [0, 1, 2, 2, 2, 2, 2, 2] }
-- nobench input { [3, 1, 7, 0, 4, 1, 6, 3] }
-- output { [3, 4, 11, 11, 15, 16, 22, 25] }
entry test_hillis = hillis_steele
\end{lstlisting}

And passes the provided tests.

\subsection{Work efficient}
The work efficient algorithm is provided below:
\begin{lstlisting}[language={futhark}]
def work_efficient [n] (xs: [n]i32) : [n]i32 = 
  let m = ilog2 n
  let upswept = 
    loop xs = copy xs for d in 0...m-1 do
      let stride = 1 << (d + 1) 
      let offset = 1 << d
      let idxs = map (\i -> i + stride - 1) (0..stride...n-1)
      -- j = i + 2^(d + 1) - 1 => j - 2^d = i + 2^d - 1, from the Guy paper
      let vals = map (\j -> xs[j - offset] + xs[j]) idxs
      in scatter xs idxs vals
  let upswept[n-1] = 0
  let downswept = 
    loop xs = upswept for d in m-1..m-2...0 do
      let stride = 1 << (d + 1) 
      let offset = 1 << d
      let (left_idxs, right_idxs) = map (\i -> 
        let left_idx = i + offset - 1
        in (left_idx, left_idx + offset)) (0..stride...n-1) |> unzip
      let left_vals = map (\r -> xs[r]) right_idxs
      -- First item is t from paper
      let right_vals = map2 (\l r -> xs[l] + xs[r]) left_idxs right_idxs
      let all_idxs = left_idxs ++ right_idxs
      let all_vals = left_vals ++ right_vals
      in scatter xs all_idxs all_vals
  in downswept
\end{lstlisting}

This algorithm is significantly more complex and attempts to closely follow the
pseudocode provided in the provided material "Prefix Sums and Their
Applications by Guy E. Blelloch".

The up-sweep first calculates the stride for the parallel computation along
with the offset. First, we calculate the indices that we are updating via a
map. Then given these indices we are able to calculate the values at that
location and at last we can do a scatter into the original array. 

The down-sweep works in somewhat the same fashion. We calculate the left- and
right child indices in an initial map. Then, given these left and right child
pairs, we calculate the values for the given positions. Here we are able to
optimise some of the computation. For example instead of recomputing $i + 2^{d
+ 1} - 1$ when finding the value for the left child, we instead realise this is
simply the same index as the right child itself. This now gives us all the left
child indices along with values and the same for the right child, so appending
the two together we can do a final scatter into the original array.

The code is tested as shown below:
\begin{lstlisting}[language={futhark}]
-- Work efficient test
-- ==
-- entry: test_efficient
-- nobench input { [0, 0, 1, 0, 0, 0, 0, 0] }
-- output { [0, 0, 0, 1, 1, 1, 1, 1] }
-- nobench input { [0, 1, 1, 0, 0, 0, 0, 0] }
-- output { [0, 0, 1, 2, 2, 2, 2, 2] }
-- nobench input { [3, 1, 7, 0, 4, 1, 6, 3] }
-- output { [0, 3, 4, 11, 11, 15, 16, 22] }
entry test_efficient = work_efficient
\end{lstlisting}

And it passes these tests.

\subsection{Benchmarking}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.95\textwidth]{../src/prefix_sum/combined-opencl.png}
  \end{center}
  \caption{Benchmark comparison of Opencl runtimes.}\label{fig:ex2}
\end{figure}

The benchmarking can be seen in Figure~\ref{fig:ex2}. Here we can see that the
built-in scan function is significantly faster than both implementations.
However, as expected the work efficient algorithm does scale better than the
hillis Steele implementation. This scaling does look like what we would expect
given the asymptotic guarantees.

